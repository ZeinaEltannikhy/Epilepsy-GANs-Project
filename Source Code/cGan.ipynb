{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example CSV loading\n",
    "data = pd.read_csv(\"/Users/mohamedeldagla/Desktop/senior year/New Trends in AI/Epilepsy - GANs/Data/Epileptic Seizure Recognition.csv\")\n",
    "\n",
    "X = data.iloc[:, 1:179].values  \n",
    "y = data.iloc[:, 179].values      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11500, 178)   (11500,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, \" \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Adjust labels from {1,2,3,4,5} to {0,1,2,3,4} if needed\n",
    "y_adj = y - 1  # so classes become 0,1,2,3,4\n",
    "y_ohe = to_categorical(y_adj, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, LeakyReLU, Dropout, Concatenate, BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "data_dim = 178       # Number of EEG features per sample\n",
    "num_classes = 5      # We have 5 classes\n",
    "latent_dim = 100     # Dimension of the noise vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "\n",
    "def build_generator(latent_dim, num_classes, data_dim):\n",
    "    \"\"\"\n",
    "    Builds a generator with multiple dense layers, batch normalization, etc.\n",
    "    \"\"\"\n",
    "    noise_input = Input(shape=(latent_dim,))\n",
    "    label_input = Input(shape=(num_classes,))\n",
    "\n",
    "    # Concatenate noise + label\n",
    "    merged_input = Concatenate()([noise_input, label_input])\n",
    "\n",
    "    x = Dense(256)(merged_input)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "\n",
    "    x = Dense(512)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "\n",
    "    x = Dense(512)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "\n",
    "    # Output layer (EEG data dimension)\n",
    "    out = Dense(data_dim, activation='linear')(x)\n",
    "\n",
    "    model = Model([noise_input, label_input], out, name=\"Generator\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Dropout, Concatenate\n",
    "\n",
    "def build_discriminator(data_dim, num_classes):\n",
    "    \"\"\"\n",
    "    Builds a more powerful discriminator with multiple hidden layers and dropout.\n",
    "    \"\"\"\n",
    "    data_input = Input(shape=(data_dim,))\n",
    "    label_input = Input(shape=(num_classes,))\n",
    "\n",
    "    # Concatenate data + label\n",
    "    merged_input = Concatenate()([data_input, label_input])\n",
    "\n",
    "    x = Dense(512)(merged_input)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(512)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Extra layer for more capacity\n",
    "    x = Dense(256)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Output (real/fake)\n",
    "    validity = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model([data_input, label_input], validity, name=\"Discriminator\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "discriminator_lr = 1e-4\n",
    "generator_lr = 2e-4\n",
    "\n",
    "# Build models\n",
    "discriminator = build_discriminator(data_dim, num_classes)\n",
    "discriminator.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(learning_rate=discriminator_lr, beta_1=0.5),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "generator = build_generator(latent_dim, num_classes, data_dim)\n",
    "\n",
    "# When we train the generator, we want the discriminator to be \"frozen\"\n",
    "# so we only update generator weights.\n",
    "discriminator.trainable = False\n",
    "\n",
    "noise = Input(shape=(latent_dim,))\n",
    "label = Input(shape=(num_classes,))\n",
    "generated_data = generator([noise, label])\n",
    "validity = discriminator([generated_data, label])\n",
    "\n",
    "combined = Model([noise, label], validity)\n",
    "combined.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(learning_rate=generator_lr, beta_1=0.5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:77: UserWarning: The model does not have any trainable weights.\n",
      "  warnings.warn(\"The model does not have any trainable weights.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [D real loss: 0.8238, D real acc: 35.94%] [D fake loss: 0.7424, D fake acc: 53.12%] [D combined loss: 0.7831, D combined acc: 44.53%] [G loss: 0.9508]\n",
      "Epoch 2/5 [D real loss: 0.8202, D real acc: 43.23%] [D fake loss: 0.7829, D fake acc: 48.05%] [D combined loss: 0.8015, D combined acc: 45.64%] [G loss: 0.9191]\n",
      "Epoch 3/5 [D real loss: 0.7964, D real acc: 45.31%] [D fake loss: 0.7741, D fake acc: 48.18%] [D combined loss: 0.7852, D combined acc: 46.74%] [G loss: 0.9168]\n",
      "Epoch 4/5 [D real loss: 0.7857, D real acc: 46.21%] [D fake loss: 0.7723, D fake acc: 47.07%] [D combined loss: 0.7790, D combined acc: 46.64%] [G loss: 0.9189]\n",
      "Epoch 5/5 [D real loss: 0.7812, D real acc: 45.31%] [D fake loss: 0.7683, D fake acc: 47.19%] [D combined loss: 0.7747, D combined acc: 46.25%] [G loss: 0.9188]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "patience = 20        # For early stopping if desired\n",
    "patience_counter = 0\n",
    "best_g_loss = np.inf\n",
    "\n",
    "# For logging if you want to plot later\n",
    "d_loss_real_list = []\n",
    "d_loss_fake_list = []\n",
    "d_loss_list = []\n",
    "g_loss_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # ---------------------\n",
    "    #  Train Discriminator\n",
    "    # ---------------------\n",
    "    # 1) Sample a real batch\n",
    "    idx = np.random.randint(0, X_scaled.shape[0], batch_size)\n",
    "    real_samples = X_scaled[idx]  # shape (batch_size, 178)\n",
    "    real_labels = y_ohe[idx]      # shape (batch_size, 5)\n",
    "\n",
    "    # 2) Generate a fake batch\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "\n",
    "    fake_classes = np.random.randint(0, num_classes, batch_size)\n",
    "    fake_labels = tf.keras.utils.to_categorical(fake_classes, num_classes)\n",
    "\n",
    "    # Use verbose=0 to suppress the \"2/2\" lines from Keras\n",
    "    gen_samples = generator.predict([noise, fake_labels], verbose=0)\n",
    "\n",
    "    # 3) Train on real\n",
    "    d_loss_real = discriminator.train_on_batch(\n",
    "        [real_samples, real_labels],\n",
    "        np.ones((batch_size, 1))\n",
    "    )\n",
    "    # 4) Train on fake\n",
    "    d_loss_fake = discriminator.train_on_batch(\n",
    "        [gen_samples, fake_labels],\n",
    "        np.zeros((batch_size, 1))\n",
    "    )\n",
    "\n",
    "    # Combine D losses\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Generator\n",
    "    # ---------------------\n",
    "    # Generator wants the discriminator to label its generated samples as real (1)\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "    fake_classes_for_gen = np.random.randint(0, num_classes, batch_size)\n",
    "    fake_labels_for_gen = tf.keras.utils.to_categorical(fake_classes_for_gen, num_classes)\n",
    "\n",
    "    valid_y = 0.9 * np.ones((batch_size, 1))  # generator wants disc to say \"real\"\n",
    "\n",
    "    # Also set verbose=0 for this predict\n",
    "    g_loss = combined.train_on_batch([noise, fake_labels_for_gen], valid_y)\n",
    "\n",
    "    # Store logs if needed for plotting\n",
    "    d_loss_real_list.append(d_loss_real[0])\n",
    "    d_loss_fake_list.append(d_loss_fake[0])\n",
    "    d_loss_list.append(d_loss[0])\n",
    "    g_loss_list.append(g_loss)\n",
    "\n",
    "    # ---------------------\n",
    "    #  Print Progress\n",
    "    # ---------------------\n",
    "    # d_loss is [loss_value, accuracy_value], so d_loss[0] is the loss,\n",
    "    # and d_loss[1] is the accuracy. Same for d_loss_real, d_loss_fake.\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{epochs} \"\n",
    "        f\"[D real loss: {d_loss_real[0]:.4f}, D real acc: {d_loss_real[1]*100:.2f}%] \"\n",
    "        f\"[D fake loss: {d_loss_fake[0]:.4f}, D fake acc: {d_loss_fake[1]*100:.2f}%] \"\n",
    "        f\"[D combined loss: {d_loss[0]:.4f}, D combined acc: {d_loss[1]*100:.2f}%] \"\n",
    "        f\"[G loss: {g_loss:.4f}]\"\n",
    "    )\n",
    "\n",
    "    # ---------------------\n",
    "    #  Early Stopping Check (optional)\n",
    "    # ---------------------\n",
    "    if g_loss < best_g_loss:\n",
    "        best_g_loss = g_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Synthetic samples shape: (2500, 178)\n",
      "Synthetic labels shape: (2500,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# How many samples you want per class\n",
    "n_new = 500\n",
    "\n",
    "# We can store the synthetic data and labels in lists or arrays\n",
    "all_synthetic_samples = []\n",
    "all_synthetic_labels = []  # integer labels, 0..4 (i.e. originally 1..5 in raw data)\n",
    "\n",
    "for class_idx in range(num_classes):\n",
    "    # Generate noise\n",
    "    noise = np.random.normal(0, 1, (n_new, latent_dim))\n",
    "    \n",
    "    # One-hot label array for this class\n",
    "    labels_one_hot = np.zeros((n_new, num_classes))\n",
    "    labels_one_hot[:, class_idx] = 1  # set the corresponding class index to 1\n",
    "\n",
    "    # Generate synthetic samples\n",
    "    synthetic_samples = generator.predict([noise, labels_one_hot])\n",
    "    \n",
    "    # (Optional) Inverse transform if you scaled the data\n",
    "    synthetic_samples = scaler.inverse_transform(synthetic_samples)\n",
    "    \n",
    "    # Store them\n",
    "    all_synthetic_samples.append(synthetic_samples)\n",
    "    \n",
    "    # Create a label array of shape (n_new,) with value `class_idx`\n",
    "    synthetic_labels_class = np.full((n_new,), class_idx)\n",
    "    all_synthetic_labels.append(synthetic_labels_class)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_synthetic_samples = np.concatenate(all_synthetic_samples, axis=0)  \n",
    "all_synthetic_labels = np.concatenate(all_synthetic_labels, axis=0)   \n",
    "\n",
    "print(\"Synthetic samples shape:\", all_synthetic_samples.shape) \n",
    "print(\"Synthetic labels shape:\", all_synthetic_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.6164285714285714\n",
      "Logistic Regression Accuracy: 0.25142857142857145\n",
      "MLP Accuracy: 0.6475\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Suppose you already have:\n",
    "#   X_scaled: shape (N, 178)  [ real features (scaled) ]\n",
    "#   y_ohe:    shape (N, 5)    [ real labels (one-hot) ]\n",
    "#   all_synthetic_samples: shape (N_syn, 178)  [ generated features ]\n",
    "#   all_synthetic_labels:  shape (N_syn,)      [ generated labels as integers 0..4 ]\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# Convert integer labels of synthetic data to one-hot\n",
    "y_syn = to_categorical(all_synthetic_labels, num_classes=5)  # shape (N_syn, 5)\n",
    "\n",
    "# Concatenate real + synthetic\n",
    "X_syn = all_synthetic_samples\n",
    "X_combined = np.concatenate((X_scaled, X_syn), axis=0)  # shape: (N + N_syn, 178)\n",
    "y_combined = np.concatenate((y_ohe, y_syn), axis=0)     # shape: (N + N_syn, 5)\n",
    "\n",
    "# Convert the one-hot labels back to integer class labels for scikit-learn\n",
    "y_classes = np.argmax(y_combined, axis=1)  # shape: (N + N_syn,)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_combined, y_classes, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 1) Random Forest\n",
    "# -------------------------\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_preds = rf.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_preds))\n",
    "\n",
    "# -------------------------\n",
    "# 2) Logistic Regression\n",
    "# -------------------------\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "lr_preds = lr.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, lr_preds))\n",
    "\n",
    "# -------------------------\n",
    "# 3) MLP\n",
    "# -------------------------\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=500, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "mlp_preds = mlp.predict(X_test)\n",
    "print(\"MLP Accuracy:\", accuracy_score(y_test, mlp_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
